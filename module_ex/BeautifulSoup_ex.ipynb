{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import sys\n",
    "import time\n",
    "\n",
    "query_txt = input('크롤링할 키워드는 무엇입니까? : ')\n",
    "f_name = input ('검색 결과를 저장할 파일 경로와 이름을 지정하세요(예:C:\\\\doit\\\\test.txt)')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#보안뉴스 소스코드 가져오기\n",
    "req = requests.get('https://www.boannews.com/Default.asp')\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "#headline0을 id로 가진 div 아래 있는 li 크롤링\n",
    "title = soup.find('div', id='headline0').find_all('li')\n",
    "\n",
    "#title에서 text 부분만 뽑아서 print\n",
    "for i in title:\n",
    "\tprint(i.text)\n",
    "    \n",
    "위의 코드에서 find는 select를 쓰면 좀 더 깔끔해진다.\n",
    "\n",
    "# id가 headline0인 태그의 자손 중 li 크롤링\n",
    "title = soup.select('#headline0 li')\n",
    " \n",
    "select는 CSS의 selector를 사용할 수 있어 더 효율적인 사용이 가능하다.\n",
    "\n",
    "#bs의 select 사용법\n",
    "\n",
    "soup.select('태그')\n",
    "soup.select('.클래스명') 혹은 ('태그.클래스명')\n",
    "soup.select('#아이디명') 혹은 ('태그#아이디명')\n",
    "soup.select('태그 > 자식태그')\n",
    "soup.select('태그 자손태그')\n",
    "find와 select의 자세한 차이점은 아래 링크에서 확인 가능하다\n",
    "\n",
    "https://stackoverflow.com/questions/38028384/beautifulsoup-is-there-a-difference-between-find-and-select-python-3-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#네이버 로그인 (안됨... 문자확인이 있어서..)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import sys\n",
    "import time\n",
    "\n",
    "path = \"c:/doit/chromedriver_win32/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(\"http://naver.com\")\n",
    "time.sleep(2)\n",
    "\n",
    "xpath = '//*[@id=\"account\"]/a'\n",
    "driver.find_element_by_xpath(xpath).click()\n",
    "\n",
    "elem_login = driver.find_element_by_id('id')\n",
    "elem_login.clear()\n",
    "elem_login.send_keys(\"shinms\")\n",
    "\n",
    "elem_login = driver.find_element_by_id('pw')\n",
    "elem_login.clear()\n",
    "elem_login.send_keys(\"msshin\")\n",
    "\n",
    "xpath = '''//*[@id=\"log.login\"]'''\n",
    "#driver.find_element_by_id('log.login')\n",
    "driver.find_element_by_xpath(xpath).click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "path = \"c:/doit/chromedriver_win32/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "url = 'https://google.com'\n",
    "driver.get(url)\n",
    "\n",
    "driver.find_element_by_css_selector('.gLFyf.gsfi').send_keys('파이썬')\n",
    "driver.find_element_by_css_selector('.gLFyf.gsfi').send_keys(Keys.ENTER)\n",
    "\n",
    "driver.find_elements_by_css_selector('.LC20lb')[2].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts.scheme:  https\n",
      "parts.netloc:  velog.io\n",
      "parts.path:  /tags/\n",
      "(parts.params:  \n",
      "parts.query:  sort=name\n",
      "parts.fragment:  \n",
      "parts:  ParseResult(scheme='https', netloc='velog.io', path='/tags/', params='', query='sort=name', fragment='')\n",
      "http\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "import urllib \n",
    "# urllib 라이브러리를 불러옵니다. \n",
    "# urllib 의 request 에서 urlopen 함수를 실행하고 결과를 r에 바인딩합니다. \n",
    "#r = urllib.request.urlopen('http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109')\n",
    "#r = urllib.request.urlopen\n",
    "#r.read().decode('utf-8')\n",
    "\n",
    "parts = urlparse('https://velog.io/tags/?sort=name')\n",
    "#parts = urllib.request.urlopen('https://velog.io/tags/?sort=name')\n",
    "\n",
    "print('parts.scheme: ',parts.scheme) # 'https'\n",
    "print('parts.netloc: ',parts.netloc) # 'velog.io:80'\n",
    "print('parts.path: ',parts.path) # '/tags/'\n",
    "print('parts.params: ', parts.params) # ''\n",
    "print('parts.query: ', parts.query) # 'sort=name'\n",
    "print('parts.fragment: ', parts.fragment) # ''\n",
    "print('parts: ', parts) # ParseResult(scheme='https', netloc='velog.io:80', path='/tags/', params='', query='sort=name', fragment='')\n",
    "\n",
    "\n",
    "#parts.scheme = 'http' # AttributeError\n",
    "parts = parts._replace(scheme='http')\n",
    "\n",
    "print(parts.scheme) # 'http'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "html = \"\"\" <!DOCTYPE html> <html> <head> <title>Page title</title> </head> <body> <div> <p>a</p> <p>b</p> <p>c</p> </div> <div class=\"ex_class\"> <p>d</p> <p>e</p> <p>f</p> </div> <div id=\"ex_id\"> <p>g</p> <p>h</p> <p>i</p> </div> <h1>This is a heading</h1> <p>This is a paragraph.</p> <p>This is another paragraph.</p> <a href=\"http://brownbears.tistory.com\" class=\"a\"/> </body> </html> \"\"\" \n",
    "bs = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>a</p>,\n",
       " <p>b</p>,\n",
       " <p>c</p>,\n",
       " <p>d</p>,\n",
       " <p>e</p>,\n",
       " <p>f</p>,\n",
       " <p>g</p>,\n",
       " <p>h</p>,\n",
       " <p>i</p>,\n",
       " <p>This is a paragraph.</p>,\n",
       " <p>This is another paragraph.</p>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.findAll('p')\n",
    "bs.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.find('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.select('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = bs.select('p')\n",
    "for i in paras :\n",
    "    print(i.text)\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p\n",
      " Hello World! \n",
      "junu\n",
      "youngone\n"
     ]
    }
   ],
   "source": [
    "tag = \"<p class='youngone' id='junu'> Hello World! </p>\" \n",
    "soup = BeautifulSoup(tag) \n",
    "object_tag = soup.find('p') \n",
    "#태그의 이름 \n",
    "object_tag.name \n",
    "#결과: 'p' \n",
    "#태그에 담긴 텍스트 \n",
    "object_tag.text \n",
    "#결과: ' Hello World! ' \n",
    "#태그의 속성과 속성값 \n",
    "object_tag.attrs \n",
    "#결과: {'class': ['youngone'], 'id': 'junu'}\n",
    "\n",
    "print (object_tag.name)\n",
    "print (object_tag.text)\n",
    "print (object_tag.attrs['id'])\n",
    "print (object_tag.attrs['class'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>a</p>\n",
      "<p>b</p>\n",
      "<p>c</p>\n",
      "<p>d</p>\n",
      "<p>e</p>\n",
      "<p>f</p>\n",
      "<p>g</p>\n",
      "<p>h</p>\n",
      "<p>i</p>\n",
      "<p>This is a paragraph.</p>\n",
      "<p>This is another paragraph.</p>\n",
      "http://brownbears.tistory.com\n",
      "http://brownbears.tistory.com\n",
      "<a class=\"a\" href=\"http://brownbears.tistory.com\"></a>\n",
      "<a class=\"a\" href=\"http://brownbears.tistory.com\"></a>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "html = \"\"\" <!DOCTYPE html> <html> <head> <title>Page title</title> </head> <body> <div> <p>a</p> <p>b</p> <p>c</p> </div> <div class=\"ex_class\"> <p>d</p> <p>e</p> <p>f</p> </div> <div id=\"ex_id\"> <p>g</p> <p>h</p> <p>i</p> </div> <h1>This is a heading</h1> <p>This is a paragraph.</p> <p>This is another paragraph.</p> <a href=\"http://brownbears.tistory.com\" class=\"a\"/> </body> </html> \"\"\" \n",
    "bs = BeautifulSoup(html, 'lxml') #'html.parser', 'lxml'\n",
    "\n",
    "for p in bs.select('p'):\n",
    "    print(p)\n",
    "    \n",
    "for link in bs.select('a'):\n",
    "    print(link.get('href')) # a태그의 href를 전부 찾기\n",
    "    print(link.attrs['href']) # 위 get 과 동일\n",
    "    \n",
    "for link in bs.find_all('a',class_='a'):\n",
    "    print(link) \n",
    "    \n",
    "for link in bs.select('a.a'): # 위 구문과 동일 결과\n",
    "    print(link) \n",
    "    import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부산 해운대 썬클라우드 호텔 스위트 오션뷰, 가성비 좋은 비지니스호텔!^^\n",
      "50달러로 시작하는 미국주식투자! (feat. 로켓컴퍼니, 알트리아, 클라우드플레어)\n",
      "시세이가구 클라우드 소파 구매 후기:)\n",
      "창원 상남동 남자 미용실 섬세한 손길 클라우드 바버샵\n",
      "AWS국비지원 : 클라우드취업 체계적으로 하는 방법은?\n",
      "피지제거기 크리스탈 클라우드 야무지게 뽑히는 피지 압출기\n",
      "서울 분위기 좋은 레스토랑 63빌딩 워킹온더클라우드 런치 후기\n",
      "포천가구단지 라클라우드에서 매트리스, 침대 골라요\n",
      "의정부 관광 호텔 / 클라우드 펜트하우스에서 파티 ♥\n",
      "클라우드 추천 두박스(DUBOX), 무료 1TB 용량으로 넉넉하게 즐기자!\n"
     ]
    }
   ],
   "source": [
    "# 네이버 블로그 주소 결과 \"q\"를 입력해줘야 함 현재 '클라우드'\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def blog_crawling(q):\n",
    "    url = 'https://search.naver.com/search.naver?where=post&sm=tab_jum&query={}'.format(q)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    for links in soup.select('li.sh_blog_top > dl'): \n",
    "        title = links.select('dt > a')\n",
    "#        title = title[0].get('title') # select로 찾은 결과는 리스트로 표시되기에 [0]을 썼습니다. \n",
    "        title = title[0].attrs['title']  \n",
    "        #        print(title)\n",
    "\n",
    "        print(title)\n",
    "\n",
    "blog_crawling('클라우드')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "url = 'https://www.coupang.com/'\n",
    "\n",
    "# 드라이버 연결\n",
    "path = \"c:/doit/chromedriver_win32/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "# 웹사이트 이동\n",
    "driver.get(url)\n",
    "time.sleep(1)\n",
    "\n",
    "# 원하는 요소(element)를 찾습니다. 이 경우에는 검색창입니다.\n",
    "search_box = driver.find_element_by_id('headerSearchKeyword')\n",
    "# 다음과 같은 동작을 하는 액션 체인을 만들었습니다.\n",
    "# 검색창을 찾고 '아이스크림'이라는 검색어를 입력한 뒤 Enter를 입력합니다.\n",
    "actions = webdriver.ActionChains(driver).send_keys_to_element(search_box, '아이스크림').send_keys(Keys.ENTER)\n",
    "# 체인을 실행합니다.\n",
    "actions.perform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5a64eb94dbc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# select information between 9th position and 4th last position\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mlink_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'http://www.website.com/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmaster_links\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink_start\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlink_end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmaster_links\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r_2 = requests.get('https://korean.visitkorea.or.kr/detail/rem_detail.html?cotid=be3db10c-b642-409c-81cc-c4cdecb5bd8b&temp=')\n",
    "\n",
    "soup = BeautifulSoup(r_2.text, 'html.parser')\n",
    "\n",
    "links = soup.find_all('a')\n",
    "\n",
    "links_list = []\n",
    "\n",
    "for link in links:\n",
    "       links_list.append(link)\n",
    "\n",
    "link_end = links_list[9:-4]\n",
    "\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html><head><title>The Dormouse's story</title></head>\n",
      "<body>\n",
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
      "and they lived at the bottom of a well.</p>\n",
      "<p class=\"story\">...</p>\n",
      "</body></html>\n",
      "******************************\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ;\n",
      "and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "#print(soup.prettify())\n",
    "print(soup)\n",
    "print(\"*\"*30)\n",
    "print(soup.prettify()) #보기좋은 html 문서로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>The Dormouse's story</title> \n",
      " ==============================\n"
     ]
    }
   ],
   "source": [
    "#제목\n",
    "print(soup.title, \"\\n\",\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "['title']\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "http://example.com/elsie\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
      "http://example.com/elsie\n",
      "http://example.com/lacie\n",
      "http://example.com/tillie\n",
      "==============================\n",
      "http://example.com/elsie\n",
      "http://example.com/lacie\n",
      "http://example.com/tillie\n"
     ]
    }
   ],
   "source": [
    "print(soup.p)\n",
    "print(soup.p['class'])\n",
    "print(soup.a)\n",
    "print(soup.a['href'])\n",
    "print(soup.find_all('a'))\n",
    "a = soup.find_all('a')\n",
    "for i in a:\n",
    "    print(i['href'])\n",
    "\n",
    "print('='*30)\n",
    "\n",
    "for link in soup.find_all('a') :\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(soup.find_all('a')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p\n",
      "['title']\n"
     ]
    }
   ],
   "source": [
    "tag = soup.p\n",
    "tag\n",
    "print(tag.name)\n",
    "print(tag['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Dormouse's story\n",
      "\n",
      "The Dormouse's story\n",
      "Once upon a time there were three little sisters; and their names were\n",
      "Elsie,\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived at the bottom of a well.\n",
      "...\n",
      "\n",
      "========================================\n",
      "\n",
      "The Dormouse's story\n",
      "\n",
      "The Dormouse's story\n",
      "Once upon a time there were three little sisters; and their names were\n",
      "Elsie,\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived at the bottom of a well.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#텍스트만 뽑아보자.\n",
    "\n",
    "print(soup.get_text())\n",
    "print(\"=\"*40)\n",
    "print(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://www.naver.com'\n",
    "response = requests.get(url)\n",
    "source = response.text\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "print(soup.select(\"#PM_ID_ct > div.header > div.section_navbar > div.area_hotkeyword.PM_CL_realtimeKeyword_base > div.ah_roll.PM_CL_realtimeKeyword_rolling_base > div > ul > li > a > span.ah_k\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "path = \"c:/doit/chromedriver_win32/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(\"http://google.com/\")\n",
    "search_box = driver.find_element_by_name(\"q\")\n",
    "search_box.send_keys(\"개발새발 블로그\")\n",
    "search_box.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 2 개의 페이지가 확인 됬습니다.\n",
      "\n",
      "서울·경기 거래절벽 “9·13 때보다 심각”\n",
      "\n",
      "‘전세 파동’ 수준…정부, 카드는 없고…\n",
      "\n",
      "\"전세가격 올려주느니 집 사자\" 서울 금관구 아파트 줄줄이 최고가\n",
      "\n",
      "\"임대료 깎아달라\" 수용의무 없는 건물주..분쟁조정위 3배 확대\n",
      "\n",
      "송파·과천 아파트 실거래 신고 ‘0’…최악의 거래절벽 오나[부동산360]\n",
      "\n",
      "귀하디 귀한 전세…이젠 수도권도 '깡통전세' 주의보\n",
      "\n",
      "\"10억 집 사면 900만원 복비\"인데… '중개인 없는 부동산 거래' 공방이 남긴 것\n",
      "\n",
      "땅파면 유물이라 개발스톱?..하남교산 '문화재 공원' 뜬다\n",
      "\n",
      "‘전세난’ 실화···강남 30평 세입자, 非강남 중형 최고가 주택 산다\n",
      "\n",
      "갭투자 후유증… 경매 넘어가 떼인 전세보증금 6년 동안 4598억\n",
      "\n",
      "[2020국감] 고층건물 밀집한 부산… 여전히 37개 동은 가연성 외장재 써\n",
      "\n",
      "\"집값·전셋값 원상회복시켜라\"…무주택 서민들의 절규\n",
      "\n",
      "[인터뷰]\"부동산 가진 자와 못 가진 자, 양극화 심화\"\n",
      "\n",
      "거리두기 완화가 상업용 부동산 살릴까… \"오피스는 기대감, 상가는 울상\"\n",
      "\n",
      "[부머 탐구생활] 청약알리미라면 2030 못지않은 ‘줍줍 청약족’\n",
      "\n",
      "[단독]경매 넘어가 못 받은 전세보증금 6년간 4597억…갭투자 후유증\n",
      "\n",
      "500가구 넘는 아파트에 ‘전세 제로’…“입주 2년차 아파트 씨 말랐다”\n",
      "\n",
      "강남3구 집값 철옹성… 지난해 재산세만 1조\n",
      "\n",
      "뾰족수 없는 전세난…정부, 24번째 대책 '만지작'\n",
      "\n",
      "엑소더스 인 서울…\"천정부지 전세값에 인천·경기로\"\n",
      "\n",
      "코로나 보다 무서운 전세난… 너도나도 '서울 탈출' 러시\n",
      "'평균 1억4,449만원' 아파트 연중 또 최고가\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "maximum = 0\n",
    "page = 1\n",
    "\n",
    "URL = 'http://land.naver.com/news/field.nhn?page=1'\n",
    "response = requests.get(URL)\n",
    "source = response.text\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "while 1:\n",
    "    page_list = soup.findAll(\"a\", {\"class\": \"NP=r:\" + str(page)})\n",
    "    if not page_list:\n",
    "        maximum = page - 1\n",
    "        break\n",
    "    page = page + 1\n",
    "print(\"총 \" + str(maximum) + \" 개의 페이지가 확인 됐습니다.\")\n",
    "\n",
    "whole_source = \"\"\n",
    "\n",
    "for page_number in range(1, maximum+1):\n",
    "    URL = 'http://land.naver.com/news/field.nhn?page=' + str(page_number)\n",
    "    response = requests.get(URL)\n",
    "    whole_source = whole_source + response.text\n",
    "\n",
    "soup = BeautifulSoup(whole_source, 'html.parser')\n",
    "find_title = soup.select(\"#content > div.section_headline > ul > li > dl > dt > a\")\n",
    "\n",
    "for title in find_title:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<a> 태그에 존재하는 모든 URL을 뽑아보자.\n",
    "import re\n",
    "links = soup.find_all('a', href =re.compile('^(http).*$'))\n",
    "for link in links :\n",
    "    if not link.text == \"\" :\n",
    "        print (link.text.strip(),' ',link.get('href')) \n",
    "#    print (link.attrs['href']) \n",
    "\n",
    "#print(link)\n",
    "\n",
    "#return bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "#    print (link.get('href'))\n",
    "# ('a', href=re.compile('^(/wiki/)((?!:).)*$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#다른거 하나더 \n",
    "s = '<META NAME=\"City\" content=\"Austin> <div class=\"question\" id=\"get attrs\" name=\"python\" x=\"something\">Hello World</div>'\n",
    "soup = BeautifulSoup(s)\n",
    "\n",
    "attributes_dictionary = soup.find('div').attrs\n",
    "print (attributes_dictionary)\n",
    "# prints: {'id': 'get attrs', 'x': 'something', 'class': ['question'], 'name': 'python'}\n",
    "\n",
    "print (attributes_dictionary['class'])\n",
    "# prints: question\n",
    "\n",
    "print (soup.find('div').get_text())\n",
    "\n",
    "metas = soup.find_all(\"meta\")\n",
    "\n",
    "for meta in metas:\n",
    "    print (meta.attrs['content'], meta.attrs['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "엑스박스와 플레이스테이션, 출시 후 이어지는 매진 행렬 뒤에는\n",
      "마인크래프트 인기 악용한 악성 앱 공격, 플레이 스토어 덮쳐\n",
      "페이스북 광고로 협박 내용 공개한 라그나로커 랜섬웨어\n",
      "엔비디아, 지포스 나우에서 발견된 고위험군 취약점 패치\n",
      "유튜브와 플레이스토어 멈췄었다... 구글 서버에 무슨 일이?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#보안뉴스 소스코드 가져오기\n",
    "req = requests.get('https://www.boannews.com/Default.asp')\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "#headline0을 id로 가진 div 아래 있는 li 크롤링\n",
    "title = soup.find('div', id='headline0').find_all('li')\n",
    "\n",
    "#title에서 text 부분만 뽑아서 print\n",
    "for i in title:\n",
    "\tprint(i.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
